{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7dUjyXtGgjaKllJi0udho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikrishnan05/Analogdevices_project-/blob/main/Unsloth_ptmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4A4G9LfFoEEg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "3e254527-68fa-42f1-8c07-f0c1cef75a97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# os is a built-in Python module that allows you to interact with the operating system.\\nimport os\\n\\n#Checking whether we are in Google Colab or not\\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\\n    print(\"Installing Unsloth for local environment...\")\\n    !pip install \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\\nelse:\\n    print(\"Installing Unsloth for Colab environment...\")\\n    #Install required libraries (packaging, ninja, einops, etc.) without installing their dependencies (because maybe Colab already has them, or to save time)\\n    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\\n    \\n    #Installing without dependencies\\n    !pip install --no-deps \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# To run this, press \"Runtime\" and press \"Run all\" on a free Tesla T4 Google Colab instance!\n",
        "\n",
        "#    Join Discord if you need help + ⭐ Star us on Github ⭐\n",
        "# To install Unsloth on your own computer, follow the installation instructions on our Github page here.\n",
        "\n",
        "# You will learn how to do data prep, how to train, how to run the model, & how to save it\n",
        "\n",
        "# News\n",
        "# Unsloth now supports Text-to-Speech (TTS) models. Read our guide here.\n",
        "\n",
        "# Read our Qwen3 Guide and check out our new Dynamic 2.0 quants which outperforms other quantization methods!\n",
        "\n",
        "# Visit our docs for all our model uploads and notebooks.\n",
        "\n",
        "# To run this, press \"Runtime\" and press \"Run all\" on a free Tesla T4 Google Colab instance!\n",
        "# %%capture # Use %%capture to hide pip outputs if desired\n",
        "\"\"\"\n",
        "# os is a built-in Python module that allows you to interact with the operating system.\n",
        "import os\n",
        "\n",
        "#Checking whether we are in Google Colab or not\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    print(\"Installing Unsloth for local environment...\")\n",
        "    !pip install \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\n",
        "else:\n",
        "    print(\"Installing Unsloth for Colab environment...\")\n",
        "    #Install required libraries (packaging, ninja, einops, etc.) without installing their dependencies (because maybe Colab already has them, or to save time)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "\n",
        "    #Installing without dependencies\n",
        "    !pip install --no-deps \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# -----------------------------------------------------------------------------\n",
        "# Cell 0.2: Additional Library Installations\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nInstalling additional libraries for data processing and DICOM handling...\")\n",
        "!pip install -q pydicom pandas opencv-python Pillow scikit-learn matplotlib seaborn \"huggingface_hub>=0.23.0\" \"hf_transfer>=0.1.6\" \"datasets>=2.16.0\" sentencepiece protobuf\n",
        "\n",
        "# Install unsloth_zoo\n",
        "print(\"\\nInstalling unsloth_zoo...\")\n",
        "!pip install unsloth_zoo\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fhJQhFboqc1U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "4ab62363-4cd2-4a96-a0e0-0eed652e3c1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# -----------------------------------------------------------------------------\\n# Cell 0.2: Additional Library Installations\\n# -----------------------------------------------------------------------------\\nprint(\"\\nInstalling additional libraries for data processing and DICOM handling...\")\\n!pip install -q pydicom pandas opencv-python Pillow scikit-learn matplotlib seaborn \"huggingface_hub>=0.23.0\" \"hf_transfer>=0.1.6\" \"datasets>=2.16.0\" sentencepiece protobuf\\n\\n# Install unsloth_zoo\\nprint(\"\\nInstalling unsloth_zoo...\")\\n!pip install unsloth_zoo\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Unsloth FastModel supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Cell 0.3: Unsloth Model Loading\n",
        "# -----------------------------------------------------------------------------\n",
        "from unsloth import FastLanguageModel # Changed from FastModel to FastLanguageModel as per recent Unsloth examples for language models\n",
        "import torch\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aTMppuNfrAxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "e09982ac-faca-41e0-ddd4-dda7400736cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Unsloth FastModel supports loading nearly any model now! This includes Vision and Text models!\\n\\n# -----------------------------------------------------------------------------\\n# Cell 0.3: Unsloth Model Loading\\n# -----------------------------------------------------------------------------\\nfrom unsloth import FastLanguageModel # Changed from FastModel to FastLanguageModel as per recent Unsloth examples for language models\\nimport torch\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Step 1: Install all necessary libraries in a single command\n",
        "# We are now installing the latest STABLE version of Unsloth from PyPI,\n",
        "# not the bleeding-edge version from GitHub. This resolves the version mismatch.\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Install the bleeding-edge developer versions of BOTH Unsloth and Transformers.\n",
        "# This syncs their features, resolving the 'StaticCache' import error for Gemma 3 models.\n",
        "\"\"\"\n",
        "# Check if on Colab\n",
        "import os\n",
        "IS_COLAB = \"COLAB_\" in \"\".join(os.environ.keys())\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"Installing latest developer versions of Unsloth and Transformers for Gemma 3 support...\")\n",
        "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" \\\n",
        "      \"transformers @ git+https://github.com/huggingface/transformers.git\" \\\n",
        "      pydicom pandas opencv-python Pillow scikit-learn matplotlib seaborn \\\n",
        "      \"huggingface_hub>=0.23.0\" \"hf_transfer>=0.1.6\" \"datasets>=2.16.0\" \\\n",
        "      sentencepiece protobuf accelerate\n",
        "else:\n",
        "    print(\"Installing Unsloth for local environment...\")\n",
        "    !pip install \"unsloth[colab-new]\"\n",
        "\n",
        "\n",
        "# Step 2: Import the libraries after installation\n",
        "# This cell should be run *after* the installation completes and the runtime has been restarted.\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(\"\\n✅ Installation and imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "try:\n",
        "    from unsloth import __version__ as unsloth_version\n",
        "    print(f\"Unsloth version: {unsloth_version}\")\n",
        "except ImportError:\n",
        "    print(\"Could not determine Unsloth version.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEG_d3uGzpTl",
        "outputId": "40f02619-7a81-4b06-fe75-c73cb5af96a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing latest developer versions of Unsloth and Transformers for Gemma 3 support...\n",
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-vw_pqp6k/unsloth_deb1635432e64e94a5f8771fa1a2d3d6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-vw_pqp6k/unsloth_deb1635432e64e94a5f8771fa1a2d3d6\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 6ac4e2e36f2f8bd0bc63a6eb85afa7097948ff3d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers@ git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-vw_pqp6k/transformers_9d159a80523441b29bfc25bdf9465500\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-vw_pqp6k/transformers_9d159a80523441b29bfc25bdf9465500\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 0687d481e2c71544501ef9cb3eef795a6e79b1de\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: hf_transfer>=0.1.6 in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.7.0)\n",
            "INFO: pip is looking at multiple versions of unsloth[colab-new] to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unsloth_zoo>=2025.7.1; extra == \"colab-new\" (from unsloth[colab-new]) (from versions: 2024.10.0, 2024.10.1, 2024.10.2, 2024.10.3, 2024.10.4, 2024.10.5, 2024.11.0, 2024.11.1, 2024.11.2, 2024.11.4, 2024.11.5, 2024.11.6, 2024.11.7, 2024.11.8, 2024.12.1, 2024.12.3, 2024.12.4, 2024.12.5, 2024.12.6, 2024.12.7, 2025.1.1, 2025.1.2, 2025.1.3, 2025.1.4, 2025.1.5, 2025.2.1, 2025.2.2, 2025.2.3, 2025.2.4, 2025.2.5, 2025.2.6, 2025.2.7, 2025.3.1, 2025.3.2, 2025.3.3, 2025.3.4, 2025.3.5, 2025.3.6, 2025.3.7, 2025.3.8, 2025.3.9, 2025.3.11, 2025.3.12, 2025.3.13, 2025.3.14, 2025.3.15, 2025.3.16, 2025.3.17, 2025.4.1, 2025.4.2, 2025.4.3, 2025.4.4, 2025.5.1, 2025.5.2, 2025.5.3, 2025.5.4, 2025.5.5, 2025.5.6, 2025.5.7, 2025.5.8, 2025.5.9, 2025.5.10, 2025.5.11, 2025.6.1, 2025.6.2, 2025.6.3, 2025.6.4, 2025.6.5, 2025.6.6, 2025.6.7, 2025.6.8)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unsloth_zoo>=2025.7.1; extra == \"colab-new\"\u001b[0m\u001b[31m\n",
            "\u001b[0m🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "\n",
            "✅ Installation and imports successful!\n",
            "PyTorch version: 2.7.0+cu126\n",
            "Unsloth version: 2025.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest public, stable versions of all necessary libraries.\n",
        "# The --upgrade flag ensures we get the newest releases from PyPI,\n",
        "# which should now be synchronized for Gemma 3 support.\n",
        "\"\"\"\n",
        "print(\"Installing latest stable versions of Unsloth and Transformers...\")\n",
        "!pip install --upgrade \"unsloth[colab-new]\" transformers accelerate\n",
        "\n",
        "# Install other packages\n",
        "!pip install -q pydicom pandas opencv-python Pillow scikit-learn matplotlib seaborn \\\n",
        "  \"huggingface_hub>=0.23.0\" \"hf_transfer>=0.1.6\" \"datasets>=2.16.0\" \\\n",
        "  sentencepiece protobuf\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "olV14GyZ79zF",
        "outputId": "a4d4e618-b10e-4dca-bc7a-c136dde43194"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing latest stable versions of Unsloth and Transformers...\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Collecting unsloth[colab-new]\n",
            "  Downloading unsloth-2025.6.12-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.6.8 (from unsloth[colab-new])\n",
            "  Downloading unsloth_zoo-2025.6.8-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: torch<=2.7.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2.6.0+cu124)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth[colab-new])\n",
            "  Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting bitsandbytes (from unsloth[colab-new])\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (24.2)\n",
            "Collecting tyro (from unsloth[colab-new])\n",
            "  Downloading tyro-0.9.26-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting datasets>=3.4.1 (from unsloth[colab-new])\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2.0.2)\n",
            "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth[colab-new])\n",
            "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.15.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.33.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.34.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new])\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth[colab-new]) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth[colab-new]) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.3.0)\n",
            "Collecting protobuf (from unsloth[colab-new])\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.6.8->unsloth[colab-new])\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.6.8->unsloth[colab-new]) (11.2.1)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.6.8->unsloth[colab-new])\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth[colab-new])\n",
            "  Downloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting torch<=2.7.0,>=2.4.0 (from unsloth[colab-new])\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting sympy>=1.13.3 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth[colab-new])\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth[colab-new]) (75.2.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth[colab-new]) (8.7.0)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from unsloth[colab-new])\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new])\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (4.4.4)\n",
            "\u001b[33mWARNING: unsloth 2025.6.12 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (3.11.15)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (2.19.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth[colab-new]) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth[colab-new]) (1.17.0)\n",
            "Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.19.1-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.6.8-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.26-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.6.12-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, transformers, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.0\n",
            "    Uninstalling transformers-4.53.0:\n",
            "      Successfully uninstalled transformers-4.53.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.46.1 cut_cross_entropy-25.1.1 datasets-3.6.0 fsspec-2025.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 protobuf-3.20.3 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 transformers-4.53.1 triton-3.3.0 trl-0.19.1 tyro-0.9.26 unsloth-2025.6.12 unsloth_zoo-2025.6.8 xformers-0.0.30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "977a1d4dbeb84bce862e42ce31419f42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.4 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: The NEW upgrade cell\n",
        "#!pip install --upgrade transformers datasets accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ACRbM7Bp4k6J",
        "outputId": "03097e43-0082-47e6-d167-0ccde1a414a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.7.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.0\n",
            "    Uninstalling transformers-4.53.0:\n",
            "      Successfully uninstalled transformers-4.53.0\n",
            "Successfully installed transformers-4.53.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "e47a617fbef1477595578786630e0c88"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: MODEL SELECTION FOR YOUR TASK\n",
        "# The model \"unsloth/gemma-3-4b-it\" is a TEXT-BASED instruct model.\n",
        "# Your original code used MedGemma, a VISION-LANGUAGE model, and processed images.\n",
        "# If your task involves processing images to predict LDL, you MUST select a vision-language model.\n",
        "# Examples:\n",
        "#   - Search for Unsloth-quantized vision models: https://huggingface.co/unsloth\n",
        "#   - Try loading a standard HF vision model (e.g., \"google/medgemma-4b-pt\", \"llava-hf/llava-1.5-7b-hf\", \"microsoft/phi-3-vision-128k-instruct\")\n",
        "#     FastLanguageModel might support them. If so, set `finetune_vision_layers = True` in the PEFT setup.\n",
        "# For this example, we'll use the text model from the Unsloth template.\n",
        "# You will need to adapt your data processing (especially image handling in the Dataset)\n",
        "# if you use a text model for a vision task, or change the model_name.\n",
        "\"\"\"\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# --- Model Selection ---\n",
        "# We are focusing on MedGemma for vision-based LDL prediction.\n",
        "selected_model_name = \"google/medgemma-4b-pt\"\n",
        "\n",
        "print(f\"Attempting to load model: {selected_model_name}\")\n",
        "# When loading a multimodal model like MedGemma, FastLanguageModel handles it.\n",
        "# The 'tokenizer' returned will be a multimodal processor (e.g., GemmaProcessor)\n",
        "# which contains both the image_processor and the text_tokenizer.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=selected_model_name,\n",
        "    max_seq_length=2048,  # Max sequence length for the language model part (less critical for pure vision regression)\n",
        "    dtype=None,           # Autodetect\n",
        "    load_in_4bit=True,    # Enable 4-bit quantization for memory efficiency\n",
        "    # token = \"hf_...\",   # Use if the model is gated\n",
        ")\n",
        "print(f\"Model {selected_model_name} loaded successfully.\")\n",
        "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
        "\n",
        "# --- Verify Image Processor and Get Vision Feature Dimension ---\n",
        "# For MedGemma, the tokenizer is a GemmaProcessor which should have an 'image_processor'\n",
        "if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "    print(\"Image processor found in tokenizer.\")\n",
        "    # The vision tower configuration is part of the main model's config for MedGemma\n",
        "    if hasattr(model.config, 'vision_config'):\n",
        "        vision_config = model.config.vision_config\n",
        "        # The vision feature dimension is typically 'hidden_size' of the vision_config\n",
        "        # For SigLIP (MedGemma's vision tower), it's usually referred to as hidden_size.\n",
        "        vision_feature_dim = vision_config.hidden_size\n",
        "        print(f\"Detected vision feature dimension from model.config.vision_config: {vision_feature_dim}\")\n",
        "    else:\n",
        "        print(\"ERROR: model.config.vision_config not found. Cannot determine vision_feature_dim automatically.\")\n",
        "        # Fallback: Try to inspect the vision_tower directly if it exists on the base model\n",
        "        # This path might be needed if Unsloth wraps the model differently.\n",
        "        base_model_ref = model.model if hasattr(model, 'model') else model\n",
        "        if hasattr(base_model_ref, 'vision_tower') and hasattr(base_model_ref.vision_tower, 'config'):\n",
        "            vision_feature_dim = base_model_ref.vision_tower.config.hidden_size\n",
        "            print(f\"Detected vision feature dimension from base_model.vision_tower.config: {vision_feature_dim}\")\n",
        "        else:\n",
        "            vision_feature_dim = None\n",
        "            print(\"ERROR: Could not access vision_tower.config. Manually inspect 'model' object and set vision_feature_dim.\")\n",
        "            print(\"Model structure:\", model) # Helps in debugging\n",
        "else:\n",
        "    print(\"ERROR: No image_processor found in the tokenizer. This is unexpected for MedGemma.\")\n",
        "    vision_feature_dim = None\n",
        "\n",
        "if vision_feature_dim is None:\n",
        "    print(\"CRITICAL ERROR: vision_feature_dim could not be determined. Regression head cannot be initialized correctly.\")\n",
        "    # You might need to manually set it based on MedGemma's architecture if auto-detection fails.\n",
        "    # For medgemma-4b-pt, the vision feature dimension (SigLIP-L/16) is 1024.\n",
        "    vision_feature_dim = 1152 # Example: Manually set if necessary\n",
        "    print(f\"Attempting to use manually set vision_feature_dim: {vision_feature_dim}\")\n",
        "\n",
        "\"\"\"\n",
        "# Note: For vision models, the 'tokenizer' might be a composite object\n",
        "# or you might access an image processor via `model.processor` or `tokenizer.image_processor`.\n",
        "# This depends on how Unsloth handles vision models.\n",
        "\"\"\"\n",
        "# Cell 0.3: Unsloth Model Loading (REVISED FOR FLOAT16)\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "selected_model_name = \"google/medgemma-4b-pt\"\n",
        "\n",
        "print(f\"Attempting to load model: {selected_model_name} with dtype=torch.float16\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=selected_model_name,\n",
        "    max_seq_length=2048,\n",
        "    dtype=torch.float16,  # <--- CRITICAL CHANGE HERE\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "print(f\"Model {selected_model_name} loaded successfully.\")\n",
        "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
        "print(f\"Base model dtype after loading: {model.dtype}\") # Should now be torch.float16\n",
        "\n",
        "# --- Verify Image Processor and Get Vision Feature Dimension ---\n",
        "# (This part remains the same as your last working version of Cell 0.3)\n",
        "if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "    print(\"Image processor found in tokenizer.\")\n",
        "    if hasattr(model.config, 'vision_config'):\n",
        "        vision_config = model.config.vision_config\n",
        "        vision_feature_dim = vision_config.hidden_size\n",
        "        print(f\"Detected vision feature dimension from model.config.vision_config: {vision_feature_dim}\")\n",
        "    else:\n",
        "        print(\"ERROR: model.config.vision_config not found. Cannot determine vision_feature_dim automatically.\")\n",
        "        base_model_ref = model.model if hasattr(model, 'model') else model\n",
        "        if hasattr(base_model_ref, 'vision_tower') and hasattr(base_model_ref.vision_tower, 'config'):\n",
        "            vision_feature_dim = base_model_ref.vision_tower.config.hidden_size\n",
        "            print(f\"Detected vision feature dimension from base_model.vision_tower.config: {vision_feature_dim}\")\n",
        "        else:\n",
        "            vision_feature_dim = None\n",
        "            print(\"ERROR: Could not access vision_tower.config. Manually inspect 'model' object and set vision_feature_dim.\")\n",
        "            # For medgemma-4b-pt, vision_feature_dim is 1024.\n",
        "            if vision_feature_dim is None:\n",
        "                print(\"Attempting to manually set vision_feature_dim to 1024 for MedGemma.\")\n",
        "                vision_feature_dim = 1024\n",
        "else:\n",
        "    print(\"ERROR: No image_processor found in the tokenizer. This is unexpected for MedGemma.\")\n",
        "    vision_feature_dim = 1024 # Fallback\n",
        "    print(f\"Attempting to manually set vision_feature_dim to {vision_feature_dim} due to missing image_processor.\")\n",
        "\n",
        "\n",
        "if vision_feature_dim is None:\n",
        "    print(\"CRITICAL ERROR: vision_feature_dim could not be determined. Regression head cannot be initialized correctly.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "6CH2bYyCacJB",
        "outputId": "4e61cbcb-8890-45dc-a4c6-d52a268b1103"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "Attempting to load model: google/medgemma-4b-pt with dtype=torch.float16\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'StaticCache' from 'transformers.models.gemma3.modeling_gemma3' (/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-426944616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to load model: {selected_model_name} with dtype=torch.float16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;31m#     dispatch_model = FastGraniteModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             return FastModel.from_pretrained(\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mmodel_name\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mmax_seq_length\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, unsloth_force_compile, *args, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mredirector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mpatch_loss_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             model_types, supports_sdpa = unsloth_compile_transformers(\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0mdtype\u001b[0m                   \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0mmodel_name\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36munsloth_compile_transformers\u001b[0;34m(dtype, model_name, model_types, token, revision, trust_remote_code, sdpa_dynamic_mask, sdpa_bool_masks, sdpa_gqa_replace, sdpa_dynamic_compile, compile_attention, disable_causal_masks, compile_torch_modules, compile_custom_modules, compile_function_calls, fuse_lm_head, gradient_checkpointing, manual_replacements, fast_lora_forwards, fast_residual_stream, accurate_accumulation, epilogue_fusion, max_autotune, shape_padding, cudagraphs, debug, fullgraph, import_from_cache, disable, return_logits, unsloth_force_compile)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0;31m# Redo patches which override compiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtemporary_patch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTEMPORARY_PATCHES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         \u001b[0mtemporary_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupports_sdpa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/temporary_patches/gemma.py\u001b[0m in \u001b[0;36mpatch_Gemma3ForConditionalGeneration_causal_mask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemma3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_gemma3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     from transformers.models.gemma3.modeling_gemma3 import (\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mStaticCache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mHybridCache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'StaticCache' from 'transformers.models.gemma3.modeling_gemma3' (/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# FINAL INSTALLATION AND MODEL LOADING SCRIPT\n",
        "# This cell does everything in one go after a full session termination.\n",
        "################################################################################\n",
        "\n",
        "# === Part 1: Installation ===\n",
        "print(\">>> STEP 1: Installing latest stable versions of all required packages...\")\n",
        "!pip install --upgrade \"unsloth[colab-new]\" transformers accelerate\n",
        "!pip install -q pydicom pandas opencv-python Pillow scikit-learn matplotlib seaborn \\\n",
        "  \"huggingface_hub>=0.23.0\" \"hf_transfer>=0.1.6\" \"datasets>=2.16.0\" \\\n",
        "  sentencepiece protobuf\n",
        "\n",
        "# === Part 2: Import and Load ===\n",
        "# We do this in the same cell after installation to ensure we use the new packages.\n",
        "# A try/except block handles the necessary runtime restart gracefully.\n",
        "print(\"\\n>>> STEP 2: Importing libraries and loading the model...\")\n",
        "import os\n",
        "import torch\n",
        "try:\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    # --- Model Loading ---\n",
        "    selected_model_name = \"google/medgemma-4b-pt\"\n",
        "    print(f\"Attempting to load model: {selected_model_name} with dtype=torch.float16\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=selected_model_name,\n",
        "        max_seq_length=2048,\n",
        "        dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    print(f\"\\n✅ SUCCESS: Model '{selected_model_name}' loaded successfully.\")\n",
        "    print(f\"Base model dtype after loading: {model.dtype}\")\n",
        "\n",
        "    # --- Vision Feature Dimension Verification ---\n",
        "    # This part will now run after the model has been successfully loaded.\n",
        "    if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "        if hasattr(model.config, 'vision_config'):\n",
        "            vision_feature_dim = model.config.vision_config.hidden_size\n",
        "            print(f\"Detected vision feature dimension from model.config.vision_config: {vision_feature_dim}\")\n",
        "        else:\n",
        "            # Fallback logic if needed, but should work with the correct library versions.\n",
        "            vision_feature_dim = 1024\n",
        "            print(f\"WARNING: model.config.vision_config not found. Manually setting vision_feature_dim to {vision_feature_dim}.\")\n",
        "    else:\n",
        "        vision_feature_dim = 1024 # Fallback\n",
        "        print(f\"CRITICAL WARNING: No image_processor found. Manually setting vision_feature_dim to {vision_feature_dim}.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"\\nCaught ImportError: {e}\")\n",
        "    print(\"\\n>>> IMPORTANT: A runtime restart is required after installation.\")\n",
        "    print(\">>> Please click 'RESTART RUNTIME' or go to 'Runtime > Restart runtime' and then run this cell again.\")\n",
        "    # This command programmatically crashes the session to force the restart button to appear.\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6JlLFXB6-f44",
        "outputId": "f3abcef1-1290-41b9-dfde-c78e6faaa6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> STEP 1: Installing latest stable versions of all required packages...\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Collecting unsloth[colab-new]\n",
            "  Downloading unsloth-2025.6.12-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.6.8 (from unsloth[colab-new])\n",
            "  Downloading unsloth_zoo-2025.6.8-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: torch<=2.7.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2.6.0+cu124)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth[colab-new])\n",
            "  Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting bitsandbytes (from unsloth[colab-new])\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (24.2)\n",
            "Collecting tyro (from unsloth[colab-new])\n",
            "  Downloading tyro-0.9.26-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting datasets>=3.4.1 (from unsloth[colab-new])\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2.0.2)\n",
            "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth[colab-new])\n",
            "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.15.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.33.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.34.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new])\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth[colab-new]) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth[colab-new]) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.3.0)\n",
            "Collecting protobuf (from unsloth[colab-new])\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.6.8->unsloth[colab-new])\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.6.8->unsloth[colab-new]) (11.2.1)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.6.8->unsloth[colab-new])\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth[colab-new])\n",
            "  Downloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting torch<=2.7.0,>=2.4.0 (from unsloth[colab-new])\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting sympy>=1.13.3 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<=2.7.0,>=2.4.0->unsloth[colab-new])\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth[colab-new])\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth[colab-new]) (75.2.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth[colab-new]) (8.7.0)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from unsloth[colab-new])\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new])\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (4.4.4)\n",
            "\u001b[33mWARNING: unsloth 2025.6.12 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (3.11.15)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (2.19.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth[colab-new]) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth[colab-new]) (1.17.0)\n",
            "Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.19.1-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.6.8-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.26-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.6.12-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, transformers, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.0\n",
            "    Uninstalling transformers-4.53.0:\n",
            "      Successfully uninstalled transformers-4.53.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.46.1 cut_cross_entropy-25.1.1 datasets-3.6.0 fsspec-2025.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 protobuf-3.20.3 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 transformers-4.53.1 triton-3.3.0 trl-0.19.1 tyro-0.9.26 unsloth-2025.6.12 unsloth_zoo-2025.6.8 xformers-0.0.30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "bad4bb763a80499bba38e6cb5a262b68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            ">>> STEP 2: Importing libraries and loading the model...\n",
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "Attempting to load model: google/medgemma-4b-pt with dtype=torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading the model with Unsloth:\n",
        "# The actual path might be model.model.vision_tower if PEFT wraps it further\n",
        "base_medgemma_model = model.model if hasattr(model, 'model') else model # Access base model if PEFT wrapped\n",
        "\n",
        "if hasattr(base_medgemma_model, 'vision_tower') and hasattr(base_medgemma_model.vision_tower, 'config'):\n",
        "    vision_config = base_medgemma_model.vision_tower.config\n",
        "    vision_feature_dim = vision_config.hidden_size\n",
        "    print(f\"Detected vision feature dimension: {vision_feature_dim}\")\n",
        "    # Now define your regression head separately or as part of a wrapper\n",
        "    # regression_head = torch.nn.Linear(vision_feature_dim, 1)\n",
        "else:\n",
        "    print(\"ERROR: Could not access model.vision_tower.config to get vision_feature_dim.\")\n",
        "    print(\"Please inspect the 'model' object structure from Unsloth carefully.\")\n",
        "    # You might need to print(model) and explore its attributes\n",
        "    vision_feature_dim = None # Fallback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7rHCq-aceho",
        "outputId": "c6bad613-8710-4ce3-8672-3ecf660784be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected vision feature dimension: 1152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Applying PEFT (LoRA) ---\")\n",
        "# `model` is the Unsloth-loaded MedGemma model from the previous cell.\n",
        "# We use get_peft_model for LoRA.\n",
        "RANDOM_SEED=42\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank (higher can mean more expressiveness but more params)\n",
        "    lora_alpha=32,  # LoRA alpha (scaling factor, often 2*r)\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\", # Recommended by Unsloth\n",
        "    random_state=RANDOM_SEED,\n",
        "    target_modules=None, # Let Unsloth automatically find layers for LoRA.\n",
        "                         # It should target both vision and language linear layers by default.\n",
        "    finetune_vision_layers=True, # CRITICAL: Ensure vision tower layers are targeted for LoRA\n",
        "    finetune_language_layers=False # OPTIONAL: For pure vision regression, we might not need to tune language layers.\n",
        "                                  # Set to False if language model outputs are not used by the regression head.\n",
        "                                  # If True (default), language LoRA adapters will also be trained.\n",
        ")\n",
        "print(\"PEFT (LoRA) adapters added to the MedGemma model.\")\n",
        "print(\"Trainable parameters after LoRA:\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APwpZTzwUoES",
        "outputId": "5b59a426-65f3-4b09-9ffc-f3a4ff894386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Applying PEFT (LoRA) ---\n",
            "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n",
            "PEFT (LoRA) adapters added to the MedGemma model.\n",
            "Trainable parameters after LoRA:\n",
            "trainable params: 8,695,296 || all params: 4,308,774,768 || trainable%: 0.2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "\n",
        "class MedGemmaVisionRegressor(nn.Module):\n",
        "    def __init__(self, peft_medgemma_model, vision_feature_dim_input: int):\n",
        "        super().__init__()\n",
        "        self.medgemma_model = peft_medgemma_model # This is the PEFT-adapted model from Unsloth\n",
        "\n",
        "        # The regression head takes the pooled vision features and outputs 1 LDL value\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(vision_feature_dim_input, vision_feature_dim_input // 2), # Intermediate layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(vision_feature_dim_input // 2, 1) # Output layer\n",
        "        )\n",
        "\n",
        "        # Note: Freezing of base MedGemma layers is handled by Unsloth's PEFT.\n",
        "        # LoRA adapters are trainable. The regression_head is also trainable.\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        # The peft_medgemma_model is already a PeftModel.\n",
        "        # We need to pass pixel_values to it.\n",
        "        # The MedGemma model's forward pass can take pixel_values directly.\n",
        "        # It will internally use its vision_tower.\n",
        "        # For regression from vision, we typically want the pooled image features.\n",
        "\n",
        "        # Option 1: If the PEFT model directly gives vision features or allows access\n",
        "        # The `Gemma3ForMultiModalGeneration` (base for MedGemma) has `vision_tower`\n",
        "        # and can output `image_embeds` or similar.\n",
        "        # When using PEFT, the base model is often accessed via `self.medgemma_model.model`\n",
        "\n",
        "        base_model = self.medgemma_model.model # Access the original model underlying PEFT\n",
        "\n",
        "        # Get vision embeddings from the vision_tower\n",
        "        # The vision_tower (SigLIP) in MedGemma outputs pooled features.\n",
        "        vision_outputs = base_model.vision_tower(pixel_values=pixel_values, return_dict=True)\n",
        "\n",
        "        # `pooler_output` from SigLipVisionModelOutput is [batch_size, vision_feature_dim]\n",
        "        image_features = vision_outputs.pooler_output\n",
        "\n",
        "        if image_features is None:\n",
        "            # Fallback if pooler_output is not directly available (should be for SigLIP)\n",
        "            # This might happen if the model structure is different than expected.\n",
        "            # For ViT-like models, the first token's embedding ([CLS] token) is often used.\n",
        "            if hasattr(vision_outputs, 'last_hidden_state'):\n",
        "                image_features = vision_outputs.last_hidden_state[:, 0, :] # CLS token embedding\n",
        "            else:\n",
        "                raise ValueError(\"Could not extract pooled image features (pooler_output or CLS token) from vision_tower output.\")\n",
        "\n",
        "        # Pass vision features through the regression head\n",
        "        ldl_prediction = self.regression_head(image_features)\n",
        "        return ldl_prediction\n",
        "\n",
        "# --- Instantiate the Regressor Model ---\n",
        "if vision_feature_dim is not None:\n",
        "    # `model` here is the PEFT-adapted MedGemma model from Cell 0.4\n",
        "    regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim)\n",
        "    print(f\"MedGemmaVisionRegressor created with regression head input dim {vision_feature_dim}.\")\n",
        "\n",
        "    # Move to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    regressor_model.to(device)\n",
        "    print(f\"Regressor model moved to {device}.\")\n",
        "\n",
        "    print(\"\\nTrainable parameters of the Regressor Model (includes LoRA + head):\")\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "    for name, param in regressor_model.named_parameters():\n",
        "        total_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "            # print(f\"Trainable: {name}, Shape: {param.shape}\") # Uncomment to see all trainable params\n",
        "    print(f\"Total parameters in RegressorModel: {total_params:,}\")\n",
        "    print(f\"Trainable parameters in RegressorModel: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "else:\n",
        "    regressor_model = None\n",
        "    print(\"CRITICAL ERROR: Cannot create MedGemmaVisionRegressor because vision_feature_dim is None.\")\n",
        "\n",
        "\"\"\"\n",
        "# Cell 0.5: Define and Instantiate Custom Model Wrapper (MedGemmaVisionRegressor) - REVISED FOR FLOAT16\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class MedGemmaVisionRegressor(nn.Module):\n",
        "    def __init__(self, peft_medgemma_model, vision_feature_dim_input: int):\n",
        "        super().__init__()\n",
        "        self.medgemma_model = peft_medgemma_model\n",
        "        self.target_dtype = self.medgemma_model.dtype # Should be torch.float16 now\n",
        "        print(f\"[Regressor Init] Base PEFT model target dtype: {self.target_dtype}\")\n",
        "\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(vision_feature_dim_input, vision_feature_dim_input // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(vision_feature_dim_input // 2, 1)\n",
        "        )\n",
        "\n",
        "        if self.target_dtype == torch.float16: # Explicitly check for float16\n",
        "            print(f\"[Regressor Init] Casting regression_head to {self.target_dtype}.\")\n",
        "            self.regression_head = self.regression_head.to(dtype=self.target_dtype)\n",
        "        elif self.target_dtype is not None: # If it's something else, print a warning but still cast\n",
        "            print(f\"[Regressor Init] WARNING: Base model dtype is {self.target_dtype}, not float16. Casting regression_head to {self.target_dtype} anyway.\")\n",
        "            self.regression_head = self.regression_head.to(dtype=self.target_dtype)\n",
        "\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        # Input pixel_values should be cast to self.target_dtype (float16) in the training loop\n",
        "\n",
        "        base_model = self.medgemma_model.model\n",
        "        vision_outputs = base_model.vision_tower(pixel_values=pixel_values, return_dict=True)\n",
        "        image_features = vision_outputs.pooler_output\n",
        "\n",
        "        if image_features is None:\n",
        "            if hasattr(vision_outputs, 'last_hidden_state'):\n",
        "                image_features = vision_outputs.last_hidden_state[:, 0, :]\n",
        "            else:\n",
        "                raise ValueError(\"Could not extract pooled image features from vision_tower output.\")\n",
        "\n",
        "        # Ensure image_features are in float16 before feeding to regression_head\n",
        "        if image_features.dtype != self.target_dtype:\n",
        "            image_features = image_features.to(self.target_dtype)\n",
        "\n",
        "        ldl_prediction = self.regression_head(image_features)\n",
        "        return ldl_prediction\n",
        "\n",
        "# --- Instantiate the Regressor Model ---\n",
        "if vision_feature_dim is not None and 'model' in locals() and model is not None:\n",
        "    regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim) # `model` is from Cell 0.3\n",
        "    print(f\"MedGemmaVisionRegressor created with regression head input dim {vision_feature_dim}.\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    regressor_model.to(device)\n",
        "    print(f\"Regressor model moved to {device}.\")\n",
        "\n",
        "    # Parameter printing (same as before)\n",
        "    print(\"\\nTrainable parameters of the Regressor Model (includes LoRA + head):\")\n",
        "    total_params = 0; trainable_params = 0\n",
        "    for name, param in regressor_model.named_parameters():\n",
        "        total_params += param.numel()\n",
        "        if param.requires_grad: trainable_params += param.numel()\n",
        "    print(f\"Total parameters in RegressorModel: {total_params:,}\")\n",
        "    print(f\"Trainable parameters in RegressorModel: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "else:\n",
        "    regressor_model = None\n",
        "    print(\"CRITICAL ERROR: Cannot create MedGemmaVisionRegressor. Check 'vision_feature_dim' and 'model'.\")"
      ],
      "metadata": {
        "id": "ZKo4mLwBchne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343ea582-daf2-4991-8bdf-517b7832d1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedGemmaVisionRegressor created with regression head input dim 1152.\n",
            "Regressor model moved to cuda.\n",
            "\n",
            "Trainable parameters of the Regressor Model (includes LoRA + head):\n",
            "Total parameters in RegressorModel: 2,499,582,961\n",
            "Trainable parameters in RegressorModel: 9,360,001 (0.37%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 1: PyTorch/HuggingFace Imports and Setup (Adapted from user's Cell 1)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nImporting libraries...\")\n",
        "# Python Standard Libraries\n",
        "import shutil # os, zipfile already imported or not needed here\n",
        "import zipfile\n",
        "\n",
        "# Third-party Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import cv2 # OpenCV\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "# import torch # Already imported\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Hugging Face (tokenizer is already loaded by Unsloth)\n",
        "# from transformers import AutoProcessor # Replaced by Unsloth's tokenizer\n",
        "\n",
        "# Plotting (optional, but often useful)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Colab specific\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"--- Library Version Checks ---\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "import sklearn\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "# print(f\"TensorFlow Version: {tf.__version__}\") # TensorFlow not used in this Unsloth/PyTorch setup\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU available for PyTorch: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU not available for PyTorch, using CPU.\")\n",
        "\n",
        "# For reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "print(\"\\nCell 1: Imports and basic setup complete.\")"
      ],
      "metadata": {
        "id": "BUr3OCOxsODW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aca0c9c-5878-4188-c1eb-1a85c7ef27a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Importing libraries...\n",
            "--- Library Version Checks ---\n",
            "Pandas version: 2.2.2\n",
            "NumPy version: 2.0.2\n",
            "Scikit-learn version: 1.6.1\n",
            "PyTorch version: 2.6.0+cu124\n",
            "PyTorch CUDA version: 12.4\n",
            "GPU available for PyTorch: Tesla T4\n",
            "\n",
            "Cell 1: Imports and basic setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# Cell 2: Configuration and Unzip Data (From user's Cell 2)\n",
        "# --------------------------------------------------\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_CSV_PATH = \"/content/drive/MyDrive/cp.csv\"\n",
        "DRIVE_ZIP_PATH = \"/content/drive/MyDrive/1000-20250517T062750Z-1-001.zip\" # Your image ZIP on Drive\n",
        "\n",
        "LOCAL_EXTRACT_PATH = \"/content/medgemma_extracted_images\"\n",
        "LOCAL_IMAGES_ROOT = os.path.join(LOCAL_EXTRACT_PATH, \"1000\") # Adjusted to match your structure\n",
        "LOCAL_CSV_PATH = \"/content/medgemma_cp.csv\"\n",
        "\n",
        "# --- Unzip Data (if not already done or if re-running) ---\n",
        "if os.path.exists(DRIVE_CSV_PATH):\n",
        "    shutil.copy(DRIVE_CSV_PATH, LOCAL_CSV_PATH)\n",
        "    print(f\"CSV copied to {LOCAL_CSV_PATH}\")\n",
        "else:\n",
        "    print(f\"ERROR: CSV file not found at {DRIVE_CSV_PATH}\")\n",
        "\n",
        "if os.path.exists(LOCAL_EXTRACT_PATH):\n",
        "    print(f\"Removing existing extraction directory: {LOCAL_EXTRACT_PATH}\")\n",
        "    shutil.rmtree(LOCAL_EXTRACT_PATH)\n",
        "os.makedirs(LOCAL_EXTRACT_PATH, exist_ok=True)\n",
        "print(f\"Created local extraction directory: {LOCAL_EXTRACT_PATH}\")\n",
        "\n",
        "if os.path.exists(DRIVE_ZIP_PATH):\n",
        "    print(f\"Unzipping {DRIVE_ZIP_PATH} to {LOCAL_EXTRACT_PATH}...\")\n",
        "    with zipfile.ZipFile(DRIVE_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_EXTRACT_PATH)\n",
        "    print(\"Unzipping complete.\")\n",
        "    if os.path.exists(LOCAL_IMAGES_ROOT):\n",
        "        print(f\"Image root folder found at: {LOCAL_IMAGES_ROOT}\")\n",
        "    else:\n",
        "        print(f\"ERROR: Expected image root folder '{LOCAL_IMAGES_ROOT}' not found after unzipping. Check ZIP structure.\")\n",
        "        print(f\"Contents of {LOCAL_EXTRACT_PATH}: {os.listdir(LOCAL_EXTRACT_PATH)}\")\n",
        "\n",
        "else:\n",
        "    print(f\"ERROR: ZIP file not found at {DRIVE_ZIP_PATH}\")\n",
        "\n",
        "print(\"\\nCell 2: Data unzipping complete.\")\n"
      ],
      "metadata": {
        "id": "qKhbFStitDQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236c435e-7b19-4312-97d1-d3c956e77862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "CSV copied to /content/medgemma_cp.csv\n",
            "Created local extraction directory: /content/medgemma_extracted_images\n",
            "Unzipping /content/drive/MyDrive/1000-20250517T062750Z-1-001.zip to /content/medgemma_extracted_images...\n",
            "Unzipping complete.\n",
            "Image root folder found at: /content/medgemma_extracted_images/1000\n",
            "\n",
            "Cell 2: Data unzipping complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# Cell 3: Load and Filter Clinical Data to create image_df (From user's Cell 3)\n",
        "# --------------------------------------------------\n",
        "image_df = pd.DataFrame()\n",
        "\n",
        "if not os.path.exists(LOCAL_CSV_PATH):\n",
        "    print(f\"FATAL ERROR: Clinical CSV file not found at the expected local path: {LOCAL_CSV_PATH}\")\n",
        "else:\n",
        "    df_raw_from_cell3 = pd.read_csv(LOCAL_CSV_PATH)\n",
        "    print(f\"Initial number of rows in clinical data (Cell 3): {len(df_raw_from_cell3)}\")\n",
        "\n",
        "    person_id_col_name_c3 = 'person_id'\n",
        "    ldl_col_name_c3 = \"LDL Cholesterol Calculation (mg/dL)\" # Ensure this matches your CSV header\n",
        "\n",
        "    if not (person_id_col_name_c3 in df_raw_from_cell3.columns and ldl_col_name_c3 in df_raw_from_cell3.columns):\n",
        "        print(f\"ERROR: Required columns ('{person_id_col_name_c3}' or '{ldl_col_name_c3}') not found in CSV.\")\n",
        "        print(f\"Available columns: {df_raw_from_cell3.columns.tolist()}\")\n",
        "    else:\n",
        "        df_selected_c3 = df_raw_from_cell3[[person_id_col_name_c3, ldl_col_name_c3]].copy()\n",
        "        df_selected_c3.rename(columns={ldl_col_name_c3: 'LDL_temp'}, inplace=True)\n",
        "        df_selected_c3['LDL_temp'] = pd.to_numeric(df_selected_c3['LDL_temp'], errors='coerce')\n",
        "        df_selected_c3.dropna(subset=['LDL_temp'], inplace=True)\n",
        "        df_selected_c3 = df_selected_c3[df_selected_c3['LDL_temp'] > 0].copy()\n",
        "        df_selected_c3[person_id_col_name_c3] = df_selected_c3[person_id_col_name_c3].astype(str)\n",
        "        print(f\"Cleaned clinical data (positive LDLs only): {len(df_selected_c3)} records.\")\n",
        "\n",
        "        ldl_lookup_c3 = df_selected_c3.set_index(person_id_col_name_c3)['LDL_temp'].to_dict()\n",
        "\n",
        "        if not (os.path.exists(LOCAL_IMAGES_ROOT) and os.path.isdir(LOCAL_IMAGES_ROOT)):\n",
        "            print(f\"FATAL ERROR: Images root path '{LOCAL_IMAGES_ROOT}' does not exist or is not a directory.\")\n",
        "        else:\n",
        "            available_folders_c3 = set(os.listdir(LOCAL_IMAGES_ROOT))\n",
        "            valid_ids_clinical_c3 = set(ldl_lookup_c3.keys())\n",
        "            common_person_ids_c3 = sorted(list(valid_ids_clinical_c3 & available_folders_c3))\n",
        "            print(f\"Found {len(common_person_ids_c3)} common person_ids for mapping.\")\n",
        "\n",
        "            image_records_list = []\n",
        "            for pid_c3 in common_person_ids_c3:\n",
        "                folder_path_c3 = os.path.join(LOCAL_IMAGES_ROOT, pid_c3)\n",
        "                ldl_val_c3 = ldl_lookup_c3[pid_c3]\n",
        "                if os.path.isdir(folder_path_c3):\n",
        "                    for filename_c3 in os.listdir(folder_path_c3):\n",
        "                        if filename_c3.lower().endswith(\".dcm\"):\n",
        "                            image_path_c3 = os.path.join(folder_path_c3, filename_c3)\n",
        "                            image_records_list.append({\n",
        "                                \"person_id\": pid_c3,\n",
        "                                \"image_path\": image_path_c3,\n",
        "                                \"LDL\": ldl_val_c3\n",
        "                            })\n",
        "            image_df = pd.DataFrame(image_records_list)\n",
        "            if not image_df.empty:\n",
        "                print(f\"Final image_df created with {len(image_df)} image-LDL pairs.\")\n",
        "                from IPython.display import display # For better display in Colab\n",
        "                display(image_df.head())\n",
        "                print(f\"LDL stats in final image_df: min={image_df['LDL'].min()}, max={image_df['LDL'].max()}, mean={image_df['LDL'].mean()}\")\n",
        "            else:\n",
        "                print(\"WARNING: image_df is empty after mapping. Check paths, IDs, and DICOM file existence.\")\n",
        "print(\"\\nCell 3: image_df preparation complete.\")"
      ],
      "metadata": {
        "id": "nyRLqIrUtLLR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "a3e93c20-a79c-40d1-e7cd-d8e48461807a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial number of rows in clinical data (Cell 3): 1067\n",
            "Cleaned clinical data (positive LDLs only): 1025 records.\n",
            "Found 527 common person_ids for mapping.\n",
            "Final image_df created with 973 image-LDL pairs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  person_id                                         image_path         LDL\n",
              "0      1002  /content/medgemma_extracted_images/1000/1002/1...  133.485054\n",
              "1      1004  /content/medgemma_extracted_images/1000/1004/1...   59.674544\n",
              "2      1004  /content/medgemma_extracted_images/1000/1004/1...   59.674544\n",
              "3      1005  /content/medgemma_extracted_images/1000/1005/1...   74.956702\n",
              "4      1007  /content/medgemma_extracted_images/1000/1007/1...   92.278412"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25be80b3-aa7f-4343-b974-48855d3ba3a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person_id</th>\n",
              "      <th>image_path</th>\n",
              "      <th>LDL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1002</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1002/1...</td>\n",
              "      <td>133.485054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1004</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1004/1...</td>\n",
              "      <td>59.674544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1004</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1004/1...</td>\n",
              "      <td>59.674544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1005</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1005/1...</td>\n",
              "      <td>74.956702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1007</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1007/1...</td>\n",
              "      <td>92.278412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25be80b3-aa7f-4343-b974-48855d3ba3a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25be80b3-aa7f-4343-b974-48855d3ba3a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25be80b3-aa7f-4343-b974-48855d3ba3a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3205cedf-0d07-475f-8116-f814b6c7057a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3205cedf-0d07-475f-8116-f814b6c7057a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3205cedf-0d07-475f-8116-f814b6c7057a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nCell 3: image_df preparation complete\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"person_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"1004\",\n          \"1007\",\n          \"1002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/content/medgemma_extracted_images/1000/1004/1004_eidon_mosaic_cfp_r_1.2.826.0.1.3680043.8.641.1.20230809.2448.36605.dcm\",\n          \"/content/medgemma_extracted_images/1000/1007/1007_eidon_mosaic_cfp_r_1.2.826.0.1.3680043.8.641.1.20230824.20355.67485.dcm\",\n          \"/content/medgemma_extracted_images/1000/1004/1004_eidon_mosaic_cfp_l_1.2.826.0.1.3680043.8.641.1.20230809.2436.96446.dcm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LDL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.751173059264335,\n        \"min\": 59.67454369,\n        \"max\": 133.4850537,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          59.67454369,\n          92.27841214,\n          133.4850537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDL stats in final image_df: min=10.77327021, max=278.5634775, mean=92.26371915419321\n",
            "\n",
            "Cell 3: image_df preparation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 4: Verify image_df (Adapted from user's Cell 4)\n",
        "# -----------------------------------------------------------------------------\n",
        "if 'image_df' in locals() and isinstance(image_df, pd.DataFrame) and not image_df.empty:\n",
        "    print(f\"\\nContinuing with 'image_df' which has {len(image_df)} records.\")\n",
        "    print(\"Columns in image_df:\", image_df.columns.tolist())\n",
        "    from IPython.display import display # Ensure display is imported\n",
        "    print(\"Sample of image_df:\")\n",
        "    display(image_df.head())\n",
        "\n",
        "    required_cols = ['person_id', 'image_path', 'LDL']\n",
        "    if not all(col in image_df.columns for col in required_cols):\n",
        "        print(f\"ERROR: 'image_df' is missing one or more required columns: {required_cols}. Please re-run Cell 3.\")\n",
        "    elif image_df['LDL'].min() <= 0:\n",
        "        print(f\"ERROR: 'image_df' still contains non-positive LDL values. LDL min: {image_df['LDL'].min()}. Please re-run filtering in Cell 3.\")\n",
        "    else:\n",
        "        print(\"'image_df' seems okay to proceed.\")\n",
        "else:\n",
        "    print(\"ERROR: 'image_df' not found or is empty. Please ensure Cell 3 (data preparation) has been run successfully.\")\n",
        "    # To prevent later errors, create an empty df if it's missing, though this indicates a problem.\n",
        "    if 'image_df' not in locals() or not isinstance(image_df, pd.DataFrame):\n",
        "        image_df = pd.DataFrame(columns=['person_id', 'image_path', 'LDL'])\n",
        "\n",
        "\n",
        "print(f\"\\nUsing Unsloth loaded model: {selected_model_name}\") # From Cell 0.3\n",
        "print(\"\\nCell 4: image_df verification and Model ID check complete.\")"
      ],
      "metadata": {
        "id": "jKystT4Bt0Yq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "76cd03b1-7657-4cd2-fb42-13857954fe82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Continuing with 'image_df' which has 973 records.\n",
            "Columns in image_df: ['person_id', 'image_path', 'LDL']\n",
            "Sample of image_df:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  person_id                                         image_path         LDL\n",
              "0      1002  /content/medgemma_extracted_images/1000/1002/1...  133.485054\n",
              "1      1004  /content/medgemma_extracted_images/1000/1004/1...   59.674544\n",
              "2      1004  /content/medgemma_extracted_images/1000/1004/1...   59.674544\n",
              "3      1005  /content/medgemma_extracted_images/1000/1005/1...   74.956702\n",
              "4      1007  /content/medgemma_extracted_images/1000/1007/1...   92.278412"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-597d324d-b694-439b-bb26-ef8c14a9069c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person_id</th>\n",
              "      <th>image_path</th>\n",
              "      <th>LDL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1002</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1002/1...</td>\n",
              "      <td>133.485054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1004</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1004/1...</td>\n",
              "      <td>59.674544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1004</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1004/1...</td>\n",
              "      <td>59.674544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1005</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1005/1...</td>\n",
              "      <td>74.956702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1007</td>\n",
              "      <td>/content/medgemma_extracted_images/1000/1007/1...</td>\n",
              "      <td>92.278412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-597d324d-b694-439b-bb26-ef8c14a9069c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-597d324d-b694-439b-bb26-ef8c14a9069c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-597d324d-b694-439b-bb26-ef8c14a9069c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dc8066ad-682d-49f7-a838-e323d02fca14\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc8066ad-682d-49f7-a838-e323d02fca14')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dc8066ad-682d-49f7-a838-e323d02fca14 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nCell 4: image_df verification and Model ID check complete\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"person_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"1004\",\n          \"1007\",\n          \"1002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/content/medgemma_extracted_images/1000/1004/1004_eidon_mosaic_cfp_r_1.2.826.0.1.3680043.8.641.1.20230809.2448.36605.dcm\",\n          \"/content/medgemma_extracted_images/1000/1007/1007_eidon_mosaic_cfp_r_1.2.826.0.1.3680043.8.641.1.20230824.20355.67485.dcm\",\n          \"/content/medgemma_extracted_images/1000/1004/1004_eidon_mosaic_cfp_l_1.2.826.0.1.3680043.8.641.1.20230809.2436.96446.dcm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LDL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.751173059264335,\n        \"min\": 59.67454369,\n        \"max\": 133.4850537,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          59.67454369,\n          92.27841214,\n          133.4850537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'image_df' seems okay to proceed.\n",
            "\n",
            "Using Unsloth loaded model: google/medgemma-4b-pt\n",
            "\n",
            "Cell 4: image_df verification and Model ID check complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 5: Unsloth Tokenizer/Processor Info (Adapted from user's Cell 5)\n",
        "# -----------------------------------------------------------------------------\n",
        "# The `medgemma_processor` is now replaced by the `tokenizer` from Unsloth.\n",
        "# For vision models, this tokenizer might wrap an image processor,\n",
        "# or `model.processor` might be set by Unsloth.\n",
        "\n",
        "# This cell's original purpose was to find TARGET_SIZE_MEDGEMMA.\n",
        "# For MedGemma, the image processor (part of the 'tokenizer' object) handles resizing.\n",
        "# We can inspect the image_processor's configuration.\n",
        "\n",
        "print(\"\\n--- Inspecting MedGemma Image Processor ---\")\n",
        "TARGET_SIZE_FOR_IMAGES = None # Will be determined by the image_processor\n",
        "\n",
        "if 'tokenizer' in locals() and hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "    medgemma_image_processor = tokenizer.image_processor\n",
        "    print(f\"MedGemma Image Processor Type: {type(medgemma_image_processor)}\")\n",
        "\n",
        "    # The image processor config usually has 'size' information.\n",
        "    # For SigLIPImageProcessor (used by MedGemma), it's often under `size` directly.\n",
        "    # The 'size' attribute can be an int (for shortest_edge) or a dict {'height': H, 'width': W}.\n",
        "    if hasattr(medgemma_image_processor, 'size'):\n",
        "        size_info = medgemma_image_processor.size\n",
        "        print(f\"  Image processor 'size' attribute: {size_info}\")\n",
        "        if isinstance(size_info, int): # e.g., size=224 means shortest edge is 224\n",
        "            # MedGemma models often use square inputs, e.g., 224x224 for SigLIP-B, 384x384 for SigLIP-L\n",
        "            # The MedGemma paper mentions images are resized to 896×896 for their experiments.\n",
        "            # However, the underlying SigLIP processor might have its own default.\n",
        "            # Let's check if 'crop_size' is also available, which is often the final input size.\n",
        "            if hasattr(medgemma_image_processor, 'crop_size') and medgemma_image_processor.crop_size is not None:\n",
        "                crop_info = medgemma_image_processor.crop_size\n",
        "                if isinstance(crop_info, int):\n",
        "                    TARGET_SIZE_FOR_IMAGES = (crop_info, crop_info)\n",
        "                elif isinstance(crop_info, dict) and 'height' in crop_info and 'width' in crop_info:\n",
        "                    TARGET_SIZE_FOR_IMAGES = (crop_info['height'], crop_info['width'])\n",
        "                print(f\"  Using 'crop_size' for TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "\n",
        "            if TARGET_SIZE_FOR_IMAGES is None: # If crop_size wasn't definitive\n",
        "                 # If size is int, assume square image based on that size for processing.\n",
        "                 # The processor itself will handle the exact resizing logic.\n",
        "                 # We use this for our basic transforms if the processor fails.\n",
        "                 TARGET_SIZE_FOR_IMAGES = (size_info, size_info)\n",
        "                 print(f\"  Using 'size' attribute for TARGET_SIZE_FOR_IMAGES (assuming square): {TARGET_SIZE_FOR_IMAGES}\")\n",
        "\n",
        "        elif isinstance(size_info, dict) and 'height' in size_info and 'width' in size_info:\n",
        "            TARGET_SIZE_FOR_IMAGES = (size_info['height'], size_info['width'])\n",
        "            print(f\"  Using 'size' dict for TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "        else:\n",
        "            print(\"  Could not determine target size from image_processor.size. Check processor config.\")\n",
        "    else:\n",
        "        print(\"  Image processor does not have a direct 'size' attribute. Check its config details.\")\n",
        "\n",
        "    # Fallback if still not found, to MedGemma paper's mentioned size\n",
        "    if TARGET_SIZE_FOR_IMAGES is None:\n",
        "        TARGET_SIZE_FOR_IMAGES = (896, 896) # Default from MedGemma paper if not found in processor\n",
        "        print(f\"  Falling back to default TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES} (from MedGemma paper)\")\n",
        "else:\n",
        "    print(\"ERROR: MedGemma image_processor not found in tokenizer. Cannot determine target image size.\")\n",
        "    TARGET_SIZE_FOR_IMAGES = (896, 896) # Fallback\n",
        "    print(f\"  Using fallback TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "\n",
        "print(f\"Final TARGET_SIZE_FOR_IMAGES to be used by Dataset (if processor fails or for reference): {TARGET_SIZE_FOR_IMAGES}\")\n",
        "print(\"\\nCell 5: MedGemma image processor check complete.\")"
      ],
      "metadata": {
        "id": "KOsK3Cd4t4G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b90abce-e91c-48ee-c583-358f7c509387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inspecting MedGemma Image Processor ---\n",
            "MedGemma Image Processor Type: <class 'transformers.models.gemma3.image_processing_gemma3.Gemma3ImageProcessor'>\n",
            "  Image processor 'size' attribute: {'height': 896, 'width': 896}\n",
            "  Using 'size' dict for TARGET_SIZE_FOR_IMAGES: (896, 896)\n",
            "Final TARGET_SIZE_FOR_IMAGES to be used by Dataset (if processor fails or for reference): (896, 896)\n",
            "\n",
            "Cell 5: MedGemma image processor check complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 6: Data Splitting (Patient-Level) and LDL Normalization (From user's Cell 6)\n",
        "# -----------------------------------------------------------------------------\n",
        "train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "ldl_scaler = None # Will store the fitted StandardScaler\n",
        "\n",
        "if 'image_df' in locals() and not image_df.empty:\n",
        "    print(f\"\\nStarting data splitting for {len(image_df)} image-LDL pairs...\")\n",
        "    if 'person_id' not in image_df.columns:\n",
        "        print(\"ERROR: 'person_id' column missing in image_df. Cannot perform patient-level split. Please check image_df preparation in Cell 3.\")\n",
        "    else:\n",
        "        unique_person_ids = image_df['person_id'].unique()\n",
        "        print(f\"Total unique patients for splitting: {len(unique_person_ids)}\")\n",
        "\n",
        "        if len(unique_person_ids) < 3: # Need at least 3 patients for train/val/test\n",
        "            print(\"Warning: Not enough unique patients for a robust 3-way (train/validation/test) split.\")\n",
        "            # Simplified split logic for few patients\n",
        "            if len(unique_person_ids) == 2:\n",
        "                train_pids, val_pids = train_test_split(unique_person_ids, test_size=0.5, random_state=RANDOM_SEED)\n",
        "                test_pids = np.array([]) # Empty array for consistency\n",
        "            elif len(unique_person_ids) == 1:\n",
        "                train_pids = unique_person_ids\n",
        "                val_pids, test_pids = np.array([]), np.array([])\n",
        "            else: # 0 patients\n",
        "                train_pids, val_pids, test_pids = np.array([]), np.array([]), np.array([])\n",
        "        else:\n",
        "            # Standard 70% train, 15% validation, 15% test split of person_ids\n",
        "            train_pids, temp_pids = train_test_split(\n",
        "                unique_person_ids, test_size=0.30, random_state=RANDOM_SEED # 70% train, 30% temp\n",
        "            )\n",
        "            if len(temp_pids) > 1 : # Ensure there's at least 2 for val/test split\n",
        "                 val_pids, test_pids = train_test_split(\n",
        "                    temp_pids, test_size=0.50, random_state=RANDOM_SEED # Split temp 50/50 for val/test (15% each of total)\n",
        "                )\n",
        "            elif len(temp_pids) == 1: # Only one patient left for temp\n",
        "                val_pids = temp_pids # Assign to validation\n",
        "                test_pids = np.array([])\n",
        "            else: # No patients left for temp\n",
        "                val_pids, test_pids = np.array([]), np.array([])\n",
        "\n",
        "\n",
        "        train_df = image_df[image_df['person_id'].isin(train_pids)].copy()\n",
        "        val_df = image_df[image_df['person_id'].isin(val_pids)].copy()\n",
        "        test_df = image_df[image_df['person_id'].isin(test_pids)].copy()\n",
        "\n",
        "        print(f\"Train set: {len(train_df)} samples from {len(train_pids)} patients.\")\n",
        "        print(f\"Validation set: {len(val_df)} samples from {len(val_pids)} patients.\")\n",
        "        print(f\"Test set: {len(test_df)} samples from {len(test_pids)} patients.\")\n",
        "\n",
        "        # Sanity check for patient overlap\n",
        "        if len(train_pids)>0 and len(val_pids)>0: assert len(set(train_pids) & set(val_pids)) == 0, \"Patient overlap train/val!\"\n",
        "        if len(train_pids)>0 and len(test_pids)>0: assert len(set(train_pids) & set(test_pids)) == 0, \"Patient overlap train/test!\"\n",
        "        if len(val_pids)>0 and len(test_pids)>0: assert len(set(val_pids) & set(test_pids)) == 0, \"Patient overlap val/test!\"\n",
        "        print(\"Patient-level splits verified (no overlap if sets are non-empty).\")\n",
        "\n",
        "        # --- LDL Value Normalization ---\n",
        "        if not train_df.empty and 'LDL' in train_df.columns:\n",
        "            print(\"\\nNormalizing LDL values using StandardScaler...\")\n",
        "            ldl_scaler = StandardScaler()\n",
        "            # Fit the scaler ONLY on the training data's LDL values\n",
        "            train_df['LDL_scaled'] = ldl_scaler.fit_transform(train_df[['LDL']])\n",
        "\n",
        "            # Transform validation and test data using the FITTED scaler\n",
        "            if not val_df.empty:\n",
        "                val_df['LDL_scaled'] = ldl_scaler.transform(val_df[['LDL']])\n",
        "            else: # Add LDL_scaled column even if empty, for consistency\n",
        "                val_df['LDL_scaled'] = pd.Series(dtype='float64')\n",
        "\n",
        "            if not test_df.empty:\n",
        "                test_df['LDL_scaled'] = ldl_scaler.transform(test_df[['LDL']])\n",
        "            else:\n",
        "                test_df['LDL_scaled'] = pd.Series(dtype='float64')\n",
        "\n",
        "            print(\"LDL normalization complete.\")\n",
        "            print(\"Scaled LDL stats in train_df (should be mean~0, std~1):\")\n",
        "            from IPython.display import display # Ensure display is imported\n",
        "            display(train_df['LDL_scaled'].describe())\n",
        "\n",
        "            # Optional: Save the scaler\n",
        "            # import joblib\n",
        "            # scaler_filename = 'ldl_scaler_medgemma.joblib'\n",
        "            # joblib.dump(ldl_scaler, scaler_filename)\n",
        "            # print(f\"LDL scaler saved to {scaler_filename}\")\n",
        "        else:\n",
        "            print(\"Train DataFrame is empty or 'LDL' column missing. Skipping LDL normalization.\")\n",
        "else:\n",
        "    print(\"image_df is empty (from Cell 3). Skipping data splitting and LDL normalization.\")\n",
        "\n",
        "print(\"\\nCell 6: Data splitting and LDL normalization attempt complete.\")"
      ],
      "metadata": {
        "id": "WkxW5lpBt-7y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "beff649b-6676-42bd-ab43-5c2e54d1f6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting data splitting for 973 image-LDL pairs...\n",
            "Total unique patients for splitting: 527\n",
            "Train set: 681 samples from 368 patients.\n",
            "Validation set: 145 samples from 79 patients.\n",
            "Test set: 147 samples from 80 patients.\n",
            "Patient-level splits verified (no overlap if sets are non-empty).\n",
            "\n",
            "Normalizing LDL values using StandardScaler...\n",
            "LDL normalization complete.\n",
            "Scaled LDL stats in train_df (should be mean~0, std~1):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    6.810000e+02\n",
              "mean     2.086763e-16\n",
              "std      1.000735e+00\n",
              "min     -2.303724e+00\n",
              "25%     -7.148712e-01\n",
              "50%     -4.975462e-02\n",
              "75%      6.670533e-01\n",
              "max      2.756859e+00\n",
              "Name: LDL_scaled, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LDL_scaled</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.810000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.086763e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000735e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-2.303724e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-7.148712e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-4.975462e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.670533e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.756859e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cell 6: Data splitting and LDL normalization attempt complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 5.1 (from user, now Cell 6.1): Check Unsloth tokenizer/model.processor\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Sanity Check for Unsloth Components (Cell 6.1) ---\")\n",
        "if 'tokenizer' in locals() and tokenizer is not None:\n",
        "    print(f\"Unsloth tokenizer IS LOADED. Type: {type(tokenizer)}\")\n",
        "    if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "        print(f\"  It has a tokenizer.image_processor of type: {type(tokenizer.image_processor)}\")\n",
        "    else:\n",
        "        print(\"  It does NOT have a direct `tokenizer.image_processor` attribute (or it's None).\")\n",
        "\n",
        "    if hasattr(model, 'processor') and model.processor is not None:\n",
        "        print(f\"Unsloth model.processor IS LOADED. Type: {type(model.processor)}\")\n",
        "        if hasattr(model.processor, 'image_processor') and model.processor.image_processor is not None:\n",
        "             print(f\"  model.processor has an image_processor component of type: {type(model.processor.image_processor)}\")\n",
        "    else:\n",
        "        print(\"  The model does NOT have a `model.processor` attribute (or it's None).\")\n",
        "\n",
        "    if not (hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None) and \\\n",
        "       not (hasattr(model, 'processor') and model.processor is not None and hasattr(model.processor, 'image_processor')):\n",
        "        print(f\"  WARNING: No obvious image processor found. The model '{selected_model_name}' may be text-only.\")\n",
        "        print(\"  If your task requires image input, ensure you've selected a vision-language model and that Unsloth loads its image processor correctly.\")\n",
        "else:\n",
        "    print(\"Unsloth tokenizer IS NOT LOADED or is None.\")"
      ],
      "metadata": {
        "id": "ExnNlqz_uEpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b02d4ce-295a-4193-a1a3-56a7718a7376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sanity Check for Unsloth Components (Cell 6.1) ---\n",
            "Unsloth tokenizer IS LOADED. Type: <class 'transformers.models.gemma3.processing_gemma3.Gemma3Processor'>\n",
            "  It has a tokenizer.image_processor of type: <class 'transformers.models.gemma3.image_processing_gemma3.Gemma3ImageProcessor'>\n",
            "  The model does NOT have a `model.processor` attribute (or it's None).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c19e4e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9632422b-6de0-449d-c15e-3d127c07972a"
      },
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 7: Custom PyTorch Dataset for DICOM Images and LDL (New Cell)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper for printing messages only once during dataset iteration or training\n",
        "printed_messages_dataset = set()\n",
        "def print_once_dataset(message):\n",
        "    global printed_messages_dataset\n",
        "    if message not in printed_messages_dataset:\n",
        "        print(message)\n",
        "        printed_messages_dataset.add(message)\n",
        "\n",
        "import torchvision.transforms as T # Import T for transforms\n",
        "\n",
        "class MedGemmaVisionDataset(Dataset):\n",
        "    def __init__(self, dataframe, medgemma_tokenizer_processor, target_img_size_ref=(896, 896)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): DataFrame with 'image_path' and 'LDL_scaled' columns.\n",
        "            medgemma_tokenizer_processor: The multimodal processor from Unsloth (contains image_processor).\n",
        "            target_img_size_ref (tuple): Reference target image size, primarily for fallback.\n",
        "                                         The image_processor itself determines the actual processing.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.processor = medgemma_tokenizer_processor # This is the GemmaProcessor (or similar)\n",
        "        self.target_size_ref = target_img_size_ref # For fallback basic transforms\n",
        "\n",
        "        if not hasattr(self.processor, 'image_processor') or self.processor.image_processor is None:\n",
        "            raise ValueError(\"The provided processor must have a valid 'image_processor' attribute for MedGemma.\")\n",
        "\n",
        "        # Basic image transforms (fallback if image_processor fails for an image)\n",
        "        self.basic_transforms = T.Compose([\n",
        "            T.Resize(self.target_size_ref),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet stats\n",
        "        ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def load_and_preprocess_dicom(self, dicom_path):\n",
        "        try:\n",
        "            dicom_file = pydicom.dcmread(dicom_path)\n",
        "            pixel_array = dicom_file.pixel_array\n",
        "\n",
        "            # Normalize pixel data to 0-255 and ensure 3 channels (RGB)\n",
        "            # This is a common pre-step before PIL conversion for many image processors\n",
        "            if pixel_array.dtype != np.uint8:\n",
        "                pixel_array = pixel_array.astype(np.float32)\n",
        "                min_val, max_val = np.min(pixel_array), np.max(pixel_array)\n",
        "                if max_val > min_val:\n",
        "                    pixel_array = (pixel_array - min_val) / (max_val - min_val) * 255.0\n",
        "                else: # Handle case where all pixels are the same\n",
        "                    pixel_array = np.zeros_like(pixel_array)\n",
        "                pixel_array = pixel_array.astype(np.uint8)\n",
        "\n",
        "            if pixel_array.ndim == 2: # Grayscale\n",
        "                pil_image = Image.fromarray(pixel_array).convert('RGB')\n",
        "            elif pixel_array.ndim == 3 and pixel_array.shape[-1] == 1: # Grayscale with channel dim\n",
        "                pil_image = Image.fromarray(pixel_array.squeeze(-1)).convert('RGB')\n",
        "            elif pixel_array.ndim == 3 and pixel_array.shape[-1] == 3: # RGB\n",
        "                pil_image = Image.fromarray(pixel_array)\n",
        "            elif pixel_array.ndim == 3 and pixel_array.shape[-1] == 4: # RGBA\n",
        "                pil_image = Image.fromarray(pixel_array).convert('RGB')\n",
        "            else:\n",
        "                print_once_dataset(f\"Warning: Unsupported DICOM pixel array shape {pixel_array.shape} for {dicom_path}. Trying to convert.\")\n",
        "                # Attempt to make it a 2D grayscale image if possible\n",
        "                if pixel_array.ndim > 2 : pixel_array = pixel_array[...,0] # take first channel or slice\n",
        "                if pixel_array.ndim > 2 : pixel_array = pixel_array[0] # take first frame\n",
        "                pil_image = Image.fromarray(pixel_array.astype(np.uint8)).convert('RGB')\n",
        "\n",
        "\n",
        "            # Use MedGemma's image_processor\n",
        "            # It expects a PIL Image or list of PIL Images.\n",
        "            # It handles resizing, normalization, and tensor conversion according to MedGemma's needs.\n",
        "            processed_output = self.processor.image_processor(images=pil_image, return_tensors=\"pt\")\n",
        "            pixel_values = processed_output.pixel_values.squeeze(0) # Remove batch dim\n",
        "            return pixel_values\n",
        "\n",
        "        except Exception as e:\n",
        "            print_once_dataset(f\"Error processing DICOM {dicom_path} with image_processor: {e}. Applying basic fallback.\")\n",
        "            # Fallback: create a dummy black image if processing fails\n",
        "            try:\n",
        "                # Try to load with PIL directly for basic transform\n",
        "                pil_image_fallback = Image.open(dicom_path).convert(\"RGB\") # This might fail for some DICOMs\n",
        "                return self.basic_transforms(pil_image_fallback)\n",
        "            except Exception as e_fallback:\n",
        "                print_once_dataset(f\"Fallback PIL loading also failed for {dicom_path}: {e_fallback}. Returning zero tensor.\")\n",
        "                return torch.zeros((3, self.target_size_ref[0], self.target_size_ref[1]))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        ldl_scaled = row['LDL_scaled'] # Target variable\n",
        "\n",
        "        pixel_values = self.load_and_preprocess_dicom(image_path)\n",
        "        target_ldl_scaled = torch.tensor(ldl_scaled, dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": target_ldl_scaled.unsqueeze(0) # Ensure target is (1,) for MSELoss\n",
        "        }\n",
        "\n",
        "# --- Create Datasets ---\n",
        "# `tokenizer` from Cell 0.3 is MedGemma's processor\n",
        "# `TARGET_SIZE_FOR_IMAGES` from Cell 5 is a reference\n",
        "if 'train_df' in locals() and not train_df.empty and 'tokenizer' in locals() and tokenizer is not None:\n",
        "    train_dataset = MedGemmaVisionDataset(train_df, tokenizer, TARGET_SIZE_FOR_IMAGES)\n",
        "    print(f\"Train dataset created with {len(train_dataset)} samples.\")\n",
        "else:\n",
        "    train_dataset = None\n",
        "    print(\"Could not create train_dataset. Check train_df and tokenizer.\")\n",
        "\n",
        "if 'val_df' in locals() and not val_df.empty and 'tokenizer' in locals() and tokenizer is not None:\n",
        "    val_dataset = MedGemmaVisionDataset(val_df, tokenizer, TARGET_SIZE_FOR_IMAGES)\n",
        "    print(f\"Validation dataset created with {len(val_dataset)} samples.\")\n",
        "else:\n",
        "    val_dataset = None\n",
        "    print(\"Could not create val_dataset. Check val_df and tokenizer.\")\n",
        "\n",
        "# Example: Fetch one item to test\n",
        "if train_dataset:\n",
        "    print(\"\\nSample from train_dataset:\")\n",
        "    try:\n",
        "        sample = train_dataset[0]\n",
        "        for key, val in sample.items():\n",
        "            print(f\"  {key}: shape {val.shape}, dtype {val.dtype}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching sample from train_dataset: {e}\")\n",
        "        print(\"This might indicate issues with DICOM loading or processing in your dataset.\")\n",
        "\n",
        "print(\"\\nCell 7: MedGemmaVisionDataset class defined and datasets instantiated.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset created with 681 samples.\n",
            "Validation dataset created with 145 samples.\n",
            "\n",
            "Sample from train_dataset:\n",
            "  pixel_values: shape torch.Size([3, 896, 896]), dtype torch.float32\n",
            "  labels: shape torch.Size([1]), dtype torch.float32\n",
            "\n",
            "Cell 7: MedGemmaVisionDataset class defined and datasets instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard PyTorch collate_fn should work if items are already tensors.\n",
        "def vision_collate_fn(batch):\n",
        "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "BATCH_SIZE = 8 # Adjust based on GPU memory (e.g., 4, 8, 16)\n",
        "\n",
        "if train_dataset:\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=vision_collate_fn, # Use custom collate\n",
        "        num_workers=2, # Use multiple workers for faster data loading if not on Windows/debugging\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    print(f\"\\nTrain DataLoader created. Batches per epoch: {len(train_loader)}\")\n",
        "else:\n",
        "    train_loader = None\n",
        "\n",
        "if val_dataset:\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False, # No need to shuffle validation data\n",
        "        collate_fn=vision_collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    print(f\"Validation DataLoader created. Batches per epoch: {len(val_loader)}\")\n",
        "else:\n",
        "    val_loader = None\n",
        "\n",
        "# Test one batch from train_loader\n",
        "if train_loader:\n",
        "    print(\"\\nSample batch from train_loader:\")\n",
        "    try:\n",
        "        batch_sample = next(iter(train_loader))\n",
        "        for key, val in batch_sample.items():\n",
        "            print(f\"  {key}: shape {val.shape}, dtype {val.dtype}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching batch from train_loader: {e}\")\n",
        "\n",
        "print(\"\\nCell 8: DataLoaders created.\")"
      ],
      "metadata": {
        "id": "cSq1JoFRvFZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f93a2b3-c611-4326-9ef1-1772e975593f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train DataLoader created. Batches per epoch: 86\n",
            "Validation DataLoader created. Batches per epoch: 19\n",
            "\n",
            "Sample batch from train_loader:\n",
            "  pixel_values: shape torch.Size([8, 3, 896, 896]), dtype torch.float32\n",
            "  labels: shape torch.Size([8, 1]), dtype torch.float32\n",
            "\n",
            "Cell 8: DataLoaders created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Training Setup (Optimizer, Loss, Learning Rate) - MODIFIED\n",
        "\n",
        "\"\"\"\n",
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 5e-5 # Common starting point for LoRA fine-tuning. May need adjustment.\n",
        "EPOCHS = 10 # Start with a moderate number, e.g., 5-20.\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "if regressor_model is not None: # Ensure the model was created in Cell 0.5\n",
        "    optimizer = optim.AdamW(regressor_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # --- STRATEGY 1: Attempt to cast the entire regressor_model to bfloat16 ---\n",
        "    # This assumes `regressor_model` is already on the correct `device` (e.g., 'cuda')\n",
        "    # And `model_dtype` (e.g. torch.bfloat16) should be what Unsloth set for the base model.\n",
        "\n",
        "    # Get the dtype from the Unsloth-loaded base model component within regressor_model\n",
        "    # This is the most reliable source for the target dtype.\n",
        "    if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'dtype'):\n",
        "        target_model_dtype = regressor_model.medgemma_model.dtype\n",
        "        print(f\"\\nTarget dtype for model components (from base Unsloth model): {target_model_dtype}\")\n",
        "\n",
        "        if target_model_dtype == torch.bfloat16:\n",
        "            print(f\"Attempting to cast entire regressor_model and its submodules to {target_model_dtype} (Strategy 1)...\")\n",
        "            try:\n",
        "                # This will attempt to cast all parameters and buffers.\n",
        "                regressor_model = regressor_model.to(dtype=target_model_dtype)\n",
        "                print(\"Casting of entire regressor_model to bfloat16 attempted.\")\n",
        "\n",
        "                # Optional: Verification - Check dtypes of some parameters\n",
        "                # print(\"Verifying some parameter dtypes after full model cast:\")\n",
        "                # for name, param in regressor_model.named_parameters():\n",
        "                #     if \"lora\" in name.lower() or \"regression_head\" in name.lower() or \"bias\" in name.lower(): # Check some key ones\n",
        "                #         if param.numel() > 0: # Only print if param is not empty\n",
        "                #             print(f\"  Param: {name[:60]}..., Dtype: {param.dtype}, Device: {param.device}\")\n",
        "                #         break # Just check a few to avoid too much output\n",
        "            except Exception as e_cast_full:\n",
        "                print(f\"ERROR during full regressor_model.to(dtype={target_model_dtype}): {e_cast_full}\")\n",
        "                print(\"Full model cast failed. Proceeding without it, relying on input tensor casting in training loop.\")\n",
        "        else:\n",
        "            print(f\"Base model dtype is {target_model_dtype}, not bfloat16. Skipping full model bfloat16 cast strategy.\")\n",
        "    else:\n",
        "        print(\"\\nCould not reliably determine target_model_dtype from regressor_model.medgemma_model.dtype.\")\n",
        "        print(\"Skipping full model cast strategy. Will rely on input tensor casting in training loop.\")\n",
        "    # --- END OF STRATEGY 1 ---\n",
        "\n",
        "    print(f\"\\nOptimizer: AdamW, LR: {LEARNING_RATE}, Weight Decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"Loss Function: MSELoss\")\n",
        "    print(f\"Training for {EPOCHS} epochs.\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: regressor_model is None (was not created in Cell 0.5). Cannot set up optimizer and loss.\")\n",
        "    optimizer = None\n",
        "    criterion = None\n",
        "\"\"\"\n",
        "\n",
        "# Cell 9: Training Setup (Optimizer, Loss, Learning Rate) - REVISED FOR FLOAT16\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "EPOCHS = 10\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "if regressor_model is not None:\n",
        "    optimizer = optim.AdamW(regressor_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'dtype'):\n",
        "        target_dtype_for_components = regressor_model.medgemma_model.dtype # Should be float16\n",
        "        print(f\"\\nTarget dtype for all model components (from base Unsloth model): {target_dtype_for_components}\")\n",
        "\n",
        "        if target_dtype_for_components == torch.float16: # Check for float16\n",
        "            print(f\"Attempting to ensure all components of regressor_model are on {target_dtype_for_components}...\")\n",
        "            try:\n",
        "                regressor_model = regressor_model.to(dtype=target_dtype_for_components)\n",
        "                print(f\"Casting of entire regressor_model to {target_dtype_for_components} completed.\")\n",
        "                # Verification\n",
        "                # print(\"Verifying select parameter dtypes after full model cast:\")\n",
        "                # for name, param in regressor_model.named_parameters():\n",
        "                #     if param.requires_grad and (\"lora\" in name.lower() or \"regression_head\" in name.lower()):\n",
        "                #         if param.numel() > 0:\n",
        "                #              print(f\"  Trainable Param: {name[:70]}..., Dtype: {param.dtype}, Device: {param.device}\")\n",
        "            except Exception as e_cast_all:\n",
        "                print(f\"ERROR during full regressor_model.to(dtype={target_dtype_for_components}): {e_cast_all}\")\n",
        "        else:\n",
        "            print(f\"Base model dtype is {target_dtype_for_components}, not float16. Current strategy might need adjustment.\")\n",
        "    else:\n",
        "        print(\"\\nCould not reliably determine target_dtype for component casting.\")\n",
        "\n",
        "    print(f\"\\nOptimizer: AdamW, LR: {LEARNING_RATE}, Weight Decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"Loss Function: MSELoss\")\n",
        "    print(f\"Training for {EPOCHS} epochs.\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: regressor_model is None. Cannot set up optimizer and loss.\")\n",
        "    optimizer = None; criterion = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYo9BAroWReB",
        "outputId": "c282279e-0f1f-4611-f584-cf3c67c29bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: AdamW, LR: 5e-05, Weight Decay: 0.01\n",
            "Loss Function: MSELoss\n",
            "Training for 10 epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Training and Evaluation Loop - MODIFIED\n",
        "\"\"\"\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "PATIENCE_EPOCHS = 3 # For early stopping if validation loss doesn't improve\n",
        "\n",
        "# Helper for printing messages only once during training loop\n",
        "printed_messages_train_loop = set() # Use a different name to avoid conflict if re-running cells\n",
        "def print_once_train_loop(message):\n",
        "    global printed_messages_train_loop\n",
        "    if message not in printed_messages_train_loop:\n",
        "        print(message)\n",
        "        printed_messages_train_loop.add(message)\n",
        "\n",
        "if regressor_model is not None and train_loader is not None and val_loader is not None and optimizer is not None and criterion is not None:\n",
        "    print(f\"\\nStarting training on device: {device}...\") # device was set in Cell 0.5\n",
        "\n",
        "    # Determine the model's expected input dtype (should be bfloat16 if Unsloth set it)\n",
        "    # This comes from the base Unsloth model component\n",
        "    if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'dtype'):\n",
        "        model_input_dtype = regressor_model.medgemma_model.dtype\n",
        "    else:\n",
        "        # Fallback if attribute not found, assume bfloat16 based on previous errors\n",
        "        print_once_train_loop(\"Warning: Could not directly get model_input_dtype from regressor_model.medgemma_model.dtype. Assuming torch.bfloat16.\")\n",
        "        model_input_dtype = torch.bfloat16\n",
        "\n",
        "    print(f\"Model's expected input dtype for pixel_values: {model_input_dtype}\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        regressor_model.train()\n",
        "        running_train_loss = 0.0\n",
        "        processed_batches_train = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            # Data from DataLoader is typically float32\n",
        "            pixel_values_f32 = batch['pixel_values'].to(device)\n",
        "            labels_f32 = batch['labels'].to(device) # Labels for MSELoss are typically Float32\n",
        "\n",
        "            # Explicitly cast pixel_values to the model's expected input dtype\n",
        "            pixel_values_casted = pixel_values_f32.to(model_input_dtype)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                # Forward pass with casted input\n",
        "                predictions = regressor_model(pixel_values_casted)\n",
        "\n",
        "                # Predictions will likely be in model_input_dtype (e.g., bfloat16).\n",
        "                # MSELoss can often handle mixed precision (e.g., bfloat16 pred, float32 label).\n",
        "                # If criterion errors on dtype, cast predictions: loss = criterion(predictions.to(torch.float32), labels_f32)\n",
        "                loss = criterion(predictions, labels_f32)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_train_loss += loss.item()\n",
        "                processed_batches_train += 1\n",
        "\n",
        "                if (i + 1) % 20 == 0 or (i + 1) == len(train_loader):\n",
        "                    print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{i+1}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print_once_train_loop(f\"ERROR during training forward/backward pass at batch {i}: {e}\")\n",
        "                if \"expected scalar type\" in str(e).lower():\n",
        "                    print_once_train_loop(f\"  Input pixel_values_casted dtype: {pixel_values_casted.dtype}\")\n",
        "                    # If predictions object exists before error:\n",
        "                    if 'predictions' in locals() and isinstance(predictions, torch.Tensor):\n",
        "                         print_once_train_loop(f\"  Predictions (if formed) dtype: {predictions.dtype}\")\n",
        "                # To get more details on where exactly the error occurs inside the model:\n",
        "                # import traceback\n",
        "                # print_once_train_loop(traceback.format_exc())\n",
        "                continue # Skip this batch and try the next one\n",
        "\n",
        "        epoch_train_loss = running_train_loss / processed_batches_train if processed_batches_train > 0 else 0.0\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Training Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        regressor_model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        processed_batches_val = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_val in val_loader:\n",
        "                pixel_values_f32_val = batch_val['pixel_values'].to(device)\n",
        "                labels_f32_val = batch_val['labels'].to(device)\n",
        "\n",
        "                pixel_values_casted_val = pixel_values_f32_val.to(model_input_dtype)\n",
        "\n",
        "                try:\n",
        "                    predictions_val = regressor_model(pixel_values_casted_val)\n",
        "                    loss_val = criterion(predictions_val, labels_f32_val)\n",
        "                    running_val_loss += loss_val.item()\n",
        "                    processed_batches_val +=1\n",
        "                except Exception as e_val:\n",
        "                    print_once_train_loop(f\"ERROR during validation forward pass: {e_val}\")\n",
        "                    continue\n",
        "\n",
        "        epoch_val_loss = running_val_loss / processed_batches_val if processed_batches_val > 0 else 0.0\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            patience_counter = 0\n",
        "            save_dir = \"./best_model_checkpoint\"\n",
        "            if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
        "\n",
        "            # Save LoRA adapters from the PEFT-adapted model component\n",
        "            if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'save_pretrained'):\n",
        "                regressor_model.medgemma_model.save_pretrained(os.path.join(save_dir, \"lora_adapters\"))\n",
        "                print(f\"Saved LoRA adapters at epoch {epoch+1}.\")\n",
        "            else:\n",
        "                print_once_train_loop(\"Could not save LoRA adapters: regressor_model.medgemma_model.save_pretrained not found.\")\n",
        "\n",
        "            # Save the state of the regression head\n",
        "            if hasattr(regressor_model, 'regression_head'):\n",
        "                torch.save(regressor_model.regression_head.state_dict(), os.path.join(save_dir, \"regression_head.pth\"))\n",
        "                print(f\"Saved regression head state at epoch {epoch+1}.\")\n",
        "            else:\n",
        "                print_once_train_loop(\"Could not save regression head: regressor_model.regression_head not found.\")\n",
        "            print(f\"Validation loss improved to {best_val_loss:.4f}. Saved best model components.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "        if patience_counter >= PATIENCE_EPOCHS:\n",
        "            print(f\"Early stopping triggered after {PATIENCE_EPOCHS} epochs without improvement on validation loss.\")\n",
        "            break\n",
        "    print(\"Training complete.\")\n",
        "else:\n",
        "    print(\"Cannot start training. One or more critical components (model, dataloaders, optimizer, criterion) are missing.\")\n",
        "\n",
        "# Plotting training and validation loss\n",
        "if train_losses and val_losses:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\"\"\"\n",
        "import torch.amp\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "PATIENCE_EPOCHS = 3\n",
        "\n",
        "printed_messages_train_loop = set()\n",
        "def print_once_train_loop(message):\n",
        "    global printed_messages_train_loop\n",
        "    if message not in printed_messages_train_loop:\n",
        "        print(message)\n",
        "        printed_messages_train_loop.add(message)\n",
        "\n",
        "if regressor_model is not None and train_loader is not None and val_loader is not None and optimizer is not None and criterion is not None:\n",
        "    print(f\"\\nStarting training on device: {device}...\")\n",
        "\n",
        "    # Model activation dtype should now be float16\n",
        "    if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'dtype'):\n",
        "        model_activation_dtype = regressor_model.medgemma_model.dtype # Should be torch.float16\n",
        "    else:\n",
        "        print_once_train_loop(\"Warning: Could not get model_activation_dtype. Assuming torch.float16.\")\n",
        "        model_activation_dtype = torch.float16\n",
        "\n",
        "    print(f\"Model's activation dtype (for input pixel_values): {model_activation_dtype}\")\n",
        "\n",
        "    # Autocast will now use float16 on CUDA for T4\n",
        "    autocast_dtype = torch.float16 # Explicitly use float16 for T4\n",
        "    print(f\"Using torch.amp.autocast with dtype: {autocast_dtype} on device type: {device.type}\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        regressor_model.train()\n",
        "        running_train_loss = 0.0\n",
        "        processed_batches_train = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            pixel_values_f32 = batch['pixel_values'].to(device)\n",
        "            labels_f32 = batch['labels'].to(device)\n",
        "\n",
        "            # Cast input to float16\n",
        "            pixel_values_casted_for_input = pixel_values_f32.to(model_activation_dtype) # model_activation_dtype is float16\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast(device_type=device.type, dtype=autocast_dtype, enabled=True):\n",
        "                try:\n",
        "                    predictions = regressor_model(pixel_values_casted_for_input)\n",
        "                    loss = criterion(predictions.to(torch.float32), labels_f32) # Keep predictions to float32 for robust loss\n",
        "                except Exception as e:\n",
        "                    print_once_train_loop(f\"ERROR during training forward pass (inside autocast) at batch {i}: {e}\")\n",
        "                    # import traceback # Uncomment for full traceback\n",
        "                    # print_once_train_loop(traceback.format_exc())\n",
        "                    continue\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            processed_batches_train += 1\n",
        "\n",
        "            if (i + 1) % 20 == 0 or (i + 1) == len(train_loader):\n",
        "                print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{i+1}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_train_loss = running_train_loss / processed_batches_train if processed_batches_train > 0 else 0.0\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Training Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "        regressor_model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        processed_batches_val = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_val in val_loader:\n",
        "                pixel_values_f32_val = batch_val['pixel_values'].to(device)\n",
        "                labels_f32_val = batch_val['labels'].to(device)\n",
        "                pixel_values_casted_for_input_val = pixel_values_f32_val.to(model_activation_dtype)\n",
        "\n",
        "                with torch.amp.autocast(device_type=device.type, dtype=autocast_dtype, enabled=True):\n",
        "                    try:\n",
        "                        predictions_val = regressor_model(pixel_values_casted_for_input_val)\n",
        "                        loss_val = criterion(predictions_val.to(torch.float32), labels_f32_val)\n",
        "                    except Exception as e_val:\n",
        "                        print_once_train_loop(f\"ERROR during validation forward pass (inside autocast): {e_val}\")\n",
        "                        continue\n",
        "                running_val_loss += loss_val.item()\n",
        "                processed_batches_val +=1\n",
        "\n",
        "        epoch_val_loss = running_val_loss / processed_batches_val if processed_batches_val > 0 else 0.0\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss; patience_counter = 0\n",
        "            save_dir = \"./best_model_checkpoint\"\n",
        "            if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
        "            if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'save_pretrained'):\n",
        "                regressor_model.medgemma_model.save_pretrained(os.path.join(save_dir, \"lora_adapters\"))\n",
        "            if hasattr(regressor_model, 'regression_head'):\n",
        "                torch.save(regressor_model.regression_head.state_dict(), os.path.join(save_dir, \"regression_head.pth\"))\n",
        "            print(f\"Validation loss improved to {best_val_loss:.4f}. Saved best model components at epoch {epoch+1}.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        print(\"-\" * 30)\n",
        "        if patience_counter >= PATIENCE_EPOCHS:\n",
        "            print(f\"Early stopping triggered.\")\n",
        "            break\n",
        "    print(\"Training complete.\")\n",
        "else:\n",
        "    print(\"Cannot start training. Critical components missing.\")\n",
        "\n",
        "if train_losses and val_losses: # Plotting\n",
        "    plt.figure(figsize=(10,5)); plt.plot(train_losses, label='Training Loss'); plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs'); plt.xlabel('Epochs'); plt.ylabel('Loss (MSE)'); plt.legend(); plt.grid(True); plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvYqs994WWHB",
        "outputId": "6de55070-d5a6-4878-89bb-443d5146d62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training on cuda...\n",
            "Model's expected dtype: torch.bfloat16\n",
            "Error during forward pass in training: expected scalar type BFloat16 but found Float\n",
            "  Input pixel_values dtype: torch.bfloat16, Model expected something else based on error.\n",
            "  Prediction output dtype (if error after some computation): N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The best model was saved during training. Here's how you might save the *final* model\n",
        "# if you didn't use early stopping or want the model from the last epoch.\n",
        "\n",
        "final_model_save_path = \"./final_model_checkpoint\"\n",
        "if regressor_model is not None and os.path.exists(\"./best_model_checkpoint\"): # Check if best model was saved\n",
        "    print(f\"\\nBest model was saved during training to ./best_model_checkpoint\")\n",
        "    print(\"To use the best model, load from './best_model_checkpoint/lora_adapters' and './best_model_checkpoint/regression_head.pth'\")\n",
        "elif regressor_model is not None: # Save final model if no best model path exists (e.g. early stopping not triggered or not implemented fully)\n",
        "    if not os.path.exists(final_model_save_path): os.makedirs(final_model_save_path)\n",
        "    print(f\"\\nSaving final model to {final_model_save_path}...\")\n",
        "    # Save LoRA adapters of the base MedGemma model\n",
        "    regressor_model.medgemma_model.save_pretrained(os.path.join(final_model_save_path, \"lora_adapters\"))\n",
        "    # Save the state of the regression head\n",
        "    torch.save(regressor_model.regression_head.state_dict(), os.path.join(final_model_save_path, \"regression_head.pth\"))\n",
        "    print(f\"Final LoRA adapters saved to {os.path.join(final_model_save_path, 'lora_adapters')}\")\n",
        "    print(f\"Final regression head state saved to {os.path.join(final_model_save_path, 'regression_head.pth')}\")\n",
        "else:\n",
        "    print(\"\\nNo model to save or best model already indicated.\")\n",
        "\n",
        "\n",
        "# --- How to load the saved (best or final) model for inference ---\n",
        "# This demonstrates loading the components back.\n",
        "\n",
        "# 1. Define the path to your saved components (e.g., best model)\n",
        "saved_lora_path = \"./best_model_checkpoint/lora_adapters\" # Or final_model_save_path + \"/lora_adapters\"\n",
        "saved_head_path = \"./best_model_checkpoint/regression_head.pth\" # Or final_model_save_path + \"/regression_head.pth\"\n",
        "\n",
        "if os.path.exists(saved_lora_path) and os.path.exists(saved_head_path) and vision_feature_dim is not None:\n",
        "    print(f\"\\n--- Example: Loading saved model components from {saved_lora_path} and {saved_head_path} ---\")\n",
        "    # A. Load the base MedGemma model (without PEFT initially, or it will try to load adapters from original HF name)\n",
        "    #    It's often cleaner to load the base and then apply PEFT adapters.\n",
        "    #    However, Unsloth's `from_pretrained` on a PEFT saved path should work.\n",
        "\n",
        "    print(f\"Loading base MedGemma model ({selected_model_name}) and then applying saved LoRA adapters from {saved_lora_path}...\")\n",
        "\n",
        "    loaded_base_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=selected_model_name, # Start with the original base model name\n",
        "        max_seq_length=2048,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "        # token = \"hf_...\"\n",
        "    )\n",
        "\n",
        "    # Now, apply the saved LoRA adapters\n",
        "    # Important: The `PeftModel.from_pretrained` expects the *base model* and the path to adapters.\n",
        "    from peft import PeftModel\n",
        "    loaded_peft_medgemma_model = PeftModel.from_pretrained(loaded_base_model, saved_lora_path)\n",
        "    print(\"PEFT MedGemma model with saved LoRA adapters loaded.\")\n",
        "\n",
        "    # B. Instantiate your RegressorModel wrapper with the loaded PEFT MedGemma\n",
        "    loaded_regressor_model = MedGemmaVisionRegressor(loaded_peft_medgemma_model, vision_feature_dim)\n",
        "\n",
        "    # C. Load the state_dict for the regression head\n",
        "    loaded_regressor_model.regression_head.load_state_dict(torch.load(saved_head_path, map_location=device))\n",
        "    print(\"Regression head state loaded.\")\n",
        "\n",
        "    loaded_regressor_model.to(device)\n",
        "    loaded_regressor_model.eval() # Set to evaluation mode\n",
        "    print(\"Complete RegressorModel loaded and ready for inference.\")\n",
        "\n",
        "    # Example inference (requires a sample from val_loader or test_loader)\n",
        "    if val_loader:\n",
        "        try:\n",
        "            sample_batch_inference = next(iter(val_loader))\n",
        "            pixel_values_inf = sample_batch_inference['pixel_values'].to(device)\n",
        "            labels_inf = sample_batch_inference['labels'].to(device)\n",
        "            with torch.no_grad():\n",
        "                predictions_inf = loaded_regressor_model(pixel_values_inf)\n",
        "            print(f\"\\nSample inference output shape: {predictions_inf.shape}\")\n",
        "            # You would then unscale predictions using ldl_scaler.inverse_transform()\n",
        "            if ldl_scaler:\n",
        "                 predicted_ldl_original_scale = ldl_scaler.inverse_transform(predictions_inf.cpu().numpy())\n",
        "                 actual_ldl_original_scale = ldl_scaler.inverse_transform(labels_inf.cpu().numpy())\n",
        "                 print(f\"Sample predictions (original scale): {predicted_ldl_original_scale[:5].flatten()}\")\n",
        "                 print(f\"Sample actuals (original scale):    {actual_ldl_original_scale[:5].flatten()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during sample inference with loaded model: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping demonstration of loading model as saved paths or vision_feature_dim not found.\")\n",
        "\n",
        "\n",
        "print(\"\\nCell 11: Model saving and loading example complete.\")\n",
        "print(\"\\n--- End of Script ---\")"
      ],
      "metadata": {
        "id": "VbHPLQdsWa2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}